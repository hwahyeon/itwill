sentiment_result<- table(result$remark)
sentiment_result
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
sentiment_result<- table(result$remark)
sentiment_result
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
library(stringr)
library(plyr)
# 1. facebook_bigdata.txt 가져오기
facebook <- file(file.choose(), encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(facebook_data) # chr [1:76]
facebook_data[1:5]
# 2. 전처리 + 형태소 생성
sentence = gsub('[[:punct:]]', '', facebook_data) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
head(sentence)
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
head(sentence)
word_list = str_split(sentence, '\\s+')# 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word_list) # unlist() : list를 vector 객체로 구조변경
length(words) # 2488
# (1) 긍정어/부정어 영어 사전 가져오기
posDic <- readLines(file.choose(), encoding = 'UTF-8') # "pos_pol_word.txt"
negDic <- readLines(file.choose(), encoding = 'UTF-8') # "neg_pol_word.txt"
# (1) 긍정어/부정어 영어 사전 가져오기
posDic <- readLines(file.choose(), encoding = 'UTF-8') # "pos_pol_word.txt"
length(posDic) # 4882
length(negDic) # 9845
# (2) 긍정어/부정어 단어 추가
posDic.final <-c(posDic, '지원')
negDic.final <-c(negDic, '불편한')
# 마지막에 단어 추가
posDic.final;  # 4883
negDic.final # 9846
negDic.final; # 9846
# (2) 감성분석을 위한 함수 정의
sentimental = function(words, posDic, negDic){
scores = laply(words, function(words, posDic, negDic) {
# 2) 단어 vs 사전 matching -> index(있음) / NA(없음)
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
# 3) 사전에 등록된 단어 추출
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
# 4) 긍정단어 합- 부정단어 합
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score) # 점수 반환
}, posDic, negDic)
scores.df = data.frame(score=scores, text=words)
return(scores.df)
}
# 4) 감성 분석 : 두번째 변수(review) 전체 레코드 대상 감성분석
result<-sentimental(words, posDic.final, negDic.final)
names(result) # "score" "text"
dim(result) # 2488   2
result$text
result$score #
# score값을 대상으로 color 칼럼 추가
result$color[result$score >=1] <- "blue"
result$color[result$score ==0] <- "green"
result$color[result$score < 0] <- "red"
# (1) 감성분석 빈도수
table(result$color)
# (2) score 칼럼 리코딩
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
result$remark[result$score < 0] <- "부정"
sentiment_result<- table(result$remark)
sentiment_result
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
#laply() : list에 외부함수를 apply시킨다는 의미.
a <- list(a=1:5)
#####################laply의 이해###################
#laply() : list에 외부함수를 apply시킨다는 의미.
a <- list(a=1:5) # 키는 a, 범위는 1:5
a #key : value
x <- function(a, b){
}
x(10,20)
laply(a, funtion(x, y, z){  #a->x, 10->y, 20->z
return(x+y+z)
}, 10, 20)
laply(a, funtion(x, y, z){  #a->x, 10->y, 20->z
return(x+y+z)
}, 10, 20)
laply(a, funtion(x, y, z){  #a->x, 10->y, 20->z
return(x+y+z)
}, 10, 20)
laply(a, funtion(x, y, z){  #a->x, 10->y, 20->z
return(x+y+z)}, 10, 20)
laply(a, funtion(x, y, z){return(x+y+z)}, 10, 20)
laply(a, funtion(x, y, z)(return(x+y+z)), 10, 20)
# 모집단 -> 표준(1,000명)
x <- rnorm(1000, mean=165.2, sd=0.5)
length(x)
hist(X)
hist(x)
# 정규성검정(H0 : 정규분포와 차이가 없다.)
shapiro.test(x)
hist(x)
hist(x)
# 1. 평균차이 검정
t.test(x, mu=165.2)
# 2. 기각역의 평균 검정
t.test(x, mu=165.1)
?t.test
t.test(x, mu=165.21)
t.test(x, mu=165.21, conf.level = 0.99) #신뢰수준 0.99
# 실습파일 가져오기
setwd("C:/ITWILL/2_Rwork/Part-III")
data <- read.csv("descriptive.csv", header=TRUE)
head(data) # 데이터셋 확인
# 1. 척도별 기술통계량
#  - 데이터 특성 보기(전체 데이터 대상)
dim(data) # 행(300)과 열(8) 정보 - 차원보기
length(data) # 열(8) 길이
length(data$survey) #survey 컬럼의 관찰치 - 행(300)
str(data) # 데이터 구조보기 -> 데이터 종류,행/열,data
str(data$survey)
# 1) 명목/서열 척도 변수의 기술통계량
# - 명목상 의미없는 수치로 표현된 변수 - 성별(gender)
length(data$gender)
summary(data$gender) # 최소,최대,중위수,평균-의미없음
table(data$gender) # 각 성별 빈도수 - outlier 확인-> 0, 5
# 데이터 특성(최소,최대,평균,분위수,노이즈-NA) 제공
summary(data)
# 1) 명목/서열 척도 변수의 기술통계량
# - 명목상 의미없는 수치로 표현된 변수 - 성별(gender)
length(data$gender)
summary(data$gender) # 최소,최대,중위수,평균-의미없음
table(data$gender) # 각 성별 빈도수 - outlier 확인-> 0, 5
data <- subset(data,data$gender == 1 | data$gender == 2) # 성별 outlier 제거
x <- table(data$gender) # 성별에 대한 빈도수 저장
x # outlier 제거 확인
barplot(x) # 범주형(명목/서열척도) 시각화 -> 막대차트
y <-  prop.table(x)
round(y*100, 2) #백분율 적용(소수점 2자리)
# 2) 등간/비율 척도 변수의 기술통계량
# - 비율척도 : 수치로 직접 입력한 변수(cost)
length(data$cost)
summary(data$cost) # 요약통계량 - 의미있음(mean) - 8.784
mean(data$cost) # NA
data$cost
# 데이터 정제 - 결측치 제거 및 outlier 제거
plot(data$cost)
data <- subset(data,data$cost >= 2 & data$cost <= 10) # 총점기준
data
x <- data$cost
x
# 2. 대푯값 : cost 변수  이용
mean(x) # 평균 : 5.354
# 평균이 극단치에 영향을 받는 경우 - 중위수(median) 대체
median(x) # 5.4
min(x)
max(x)
range(x) # min ~ max
sort(x) # 오름차순
sort(x, decreasing=T) # 내림차순
dim(data)
#data.frame 정렬 : 특정(cost)
data[sort(data$cost),]
#data.frame 정렬 : 특정(cost)
head(data[sort(data$cost),])
#data.frame 정렬 : 특정(cost)
head(data[sort(data$cost),],30)
head(data[order(data$cost),],30)
library(dplyr)
arrange(data, cost) #오름차순
mode(x)
install.packages("prettyR")
library(prettyR)
Mode(x)
mean(x)
median(x)
# 3. 산포도 : cost 변수 이용
var(x) # 분산
sd(x) # 표준편차는 분t산의 양의 제곱근
quantile(x, 1/4) # 1 사분위수
quantile(x, 3/4) # 3 사분위수
min(x) # 최소값
max(x) # 최대값
range(x) # 범위(min ~ max)
# 4. 비대칭도 :  패키지 이용
install.packages("moments")  # 왜도/첨도 위한 패키지 설치
library(moments)
cost <- data$cost # 정제된 data
cost
# 왜도 - 평균을 중심으로 기울어진 정도
skewness(cost)
# 첨도 - 표준정규분포와 비교하여 얼마나 뽀족한가 측정 지표
kurtosis(cost)
# 기본 히스토그램
hist(cost)
# 히스토그램 확률밀도/표준정규분포 곡선
hist(cost, freq = F)
# 확률밀도 분포 곡선 : 히스토그램의 밀도 추정
lines(density(cost), col='blue')
# 표준정규분포 곡선
x <- seq(0, 8, 0.1)
curve( dnorm(x, mean(cost), sd(cost)), col='red', add = T)
# 정규성 검정
shapiro.test(cost)
# 1) 거주지역
data$resident2[data$resident == 1] <-"특별시"
data$resident2[data$resident >=2 & data$resident <=4] <-"광역시"
data$resident2[data$resident == 5] <-"시구군"
x<- table(data$resident2)
x
prop.table(x) # 비율 계산 : 0< x <1 사이의 값
y <-  prop.table(x)
round(y*100, 2) #백분율 적용(소수점 2자리)
setwd("C:/ITWILL/2_Rwork/Part-III")
data <- read.csv("descriptive.csv", header=TRUE)
head(data) # 데이터셋 확인
hist(data$pass)
type <- data$type
pass <- data$pass
type <- data$type
pass <- data$pass
t1 <- table(type)
barplot(t1)
t2 <- table(pass)
barplot(t2)
# 1. 패키지 설치
install.packages('httr')
library(httr)
install.packages('XML')
library(XML)
# 2. url 요청
url <- "https://media.daum.net"
GET(url)
web <- GET(url)
web <- GET(url)
# 3. html 파싱(text -> html)
help("htmlTreeParse")
html <- xmlTreeParse(web, useInternalNodes = TRUE, #노드를 사용하겠다.
trim = T, #앞 뒤 공백제거
encoding = "utf-8")
html <- htmlTreeParse(web, useInternalNodes = TRUE, #노드를 사용하겠다.
trim = T, #앞 뒤 공백제거
encoding = "utf-8")
html
html <- xmlRoot(html)
html <- htmlTreeParse(web, useInternalNodes = TRUE, #노드를 사용하겠다.
trim = T, #앞 뒤 공백제거
encoding = "utf-8")
root_html <- xmlRoot(html)
# 4. tag 자료수집 : "//tag[@속성='값']"
news <- xpathSApply(root_node, "//a[@class='link_txt']", xmlValue)
root_node <- xmlRoot(html) #루트문
# 4. tag 자료수집 : "//tag[@속성='값']"
news <- xpathSApply(root_node, "//a[@class='link_txt']", xmlValue)
news
news2 <- news[1:50]
news2
news_sent = gsub('[\n\r\t]', '', news2) #이스케이프 제거
news_sent = gsub('[[:punct:]]', '', news_sent) #문장부호 제거
news_sent = gsub('[[:cnrtl:]]', '', news_sent) #특수문자 제거
news_sent = gsub('[a-z]', '', news_sent) #영문 제거
news_sent = gsub('[A-Z]', '', news_sent) #대문자 제거
news_sent = gsub('\\s+', '', news_sent) # 2개 이상 공백 제거(특수문자와 영문을 지우면서 생긴 공백)
# 5. news 전처리
news_sent = gsub('[\n\r\t]', '', news2) #이스케이프 제거
news_sent = gsub('[[:punct:]]', '', news_sent) #문장부호 제거
news_sent = gsub('[[:cntrl:]]', '', news_sent) #특수문자 제거
news_sent = gsub('[a-z]', '', news_sent) #영문 제거
news_sent = gsub('[A-Z]', '', news_sent) #대문자 제거
news_sent = gsub('\\s+', '', news_sent) # 2개 이상 공백 제거(특수문자와 영문을 지우면서 생긴 공백)
news_sent
# 6. file save
setwd("C:/ITWILL/2_Rwork/output")
# 행 번호와 텍스트 저장, ""은 제거
write.csv(news_sent, 'news_data.csv', row.names = T, quote = F)
news_data <- read.csv('news_data.csv')
head(news_data)
colnames(news_data) <- c('no', 'news_text')
head(news_data)
news_text <- news_data$news_text
news_text
# 7. 토픽분석 -> 단어구름 시각화(1day)
library(KoNLP)
library(tm)
library(wordcloud)
#신규단어
user_dic = data.frame(term = c("펜데믹", "코로나19", "타다"),
tag='ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
extractNoun("우리나라는 현재 코로나19 때문에 공황상태이다.")
## [4. 단어추출 사용자 함수 정의]
# (1) 사용자 정의 함수 실행 순서 : 문장 -> 문자형 -> 명사 추출 -> 공백 합침
exNouns <- function(x) {
paste(extractNoun(as.character(x)), collapse=" ")
}
# (2) exNouns 함수 이용 단어 추출
# 형식) sapply(vector, 함수) -> 문장에서 단어 추출
news_text <- sapply(news_text, exNouns)
news_text
# (3) 단어 추출 결과
str(news_text)
news_text[2]
## 데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(news_text))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
# (2) 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, tolower) # 소문자 변경
myCorpus
myCorpus
# (2) 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 6. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(한글 1개 2byte) 단어 선별 -> matrix 변경
DocumentTermMatrix() #  DTM
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(4,16))) # 2절~8절
# TermDocumentMatrix : [Term, Doc]
myCorpusPrepro_term
# (2) Corpus -> 평서문 변환 : matrix -> data.frame 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
str(myTerm_df)
myTerm_df[c(1:10), 1]
## 7. 단어 빈도수 구하기
# (1) 단어 빈도수 내림차순 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
w_name <- names(wordResult)
w_name[1:10] # 단어이름
wordResult[1:10] # 출현빈도
# (2) 불용어 제거 : 의미없는 단어 제거
# [단계1] 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
# [단계2] 단어문서 행렬 생성
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(4,16))) # 2절~8절
# [단계3] 말뭉치 -> 평서문
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
# [단계4] 내림정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
## 8. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal, family="malgun")
html <- htmlTreeParse(web, useInternalNodes = TRUE, #노드를 사용하겠다.
trim = T, #앞 뒤 공백제거
encoding = "utf-8")
root_node <- xmlRoot(html) #루트문
# 4. tag 자료수집 : "//tag[@속성='값']"
news <- xpathSApply(root_node, "//a[@class='link_txt']", xmlValue)
news
news2 <- news[1:50]
news2
# 5. news 전처리
news_sent = gsub('[\n\r\t]', '', news2) #이스케이프 제거
news_sent = gsub('[[:punct:]]', '', news_sent) #문장부호 제거
news_sent = gsub('[[:cntrl:]]', '', news_sent) #특수문자 제거
news_sent = gsub('[a-z]', '', news_sent) #영문 제거
news_sent = gsub('[A-Z]', '', news_sent) #대문자 제거
news_sent = gsub('\\s+', ' ', news_sent) # 2개 이상 공백 제거(특수문자와 영문을 지우면서 생긴 공백)
news_sent
# 6. file save
setwd("C:/ITWILL/2_Rwork/output")
# 행 번호와 텍스트 저장, ""은 제거
write.csv(news_sent, 'news_data.csv', row.names = T, quote = F)
news_data <- read.csv('news_data.csv')
head(news_data)
colnames(news_data) <- c('no', 'news_text')
head(news_data)
news_text <- news_data$news_text
news_text
#신규단어
user_dic = data.frame(term = c("펜데믹", "코로나19", "타다"),
tag='ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
extractNoun("우리나라는 현재 코로나19 때문에 공황상태이다.")
# (2) exNouns 함수 이용 단어 추출
# 형식) sapply(vector, 함수) -> 문장에서 단어 추출
news_text <- sapply(news_text, exNouns)
news_text
# 단어 추출 결과
str(news_text)
news_text[2]
## 데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(news_text))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 6. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(한글 1개 2byte) 단어 선별 -> matrix 변경
DocumentTermMatrix() #  DTM
## 데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(news_text))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(4,16))) # 2절~8절
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
# TermDocumentMatrix : [Term, Doc]
myCorpusPrepro_term
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
# (2) 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, tolower) # 소문자 변경
myCorpusPrepro <-tm_map(myCorpusPrepro, removeWords, stopwords('english')) # 불용어제거
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 6. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(한글 1개 2byte) 단어 선별 -> matrix 변경
DocumentTermMatrix() #  DTM
TermDocumentMatrix() #  TDM
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(4,16))) # 2절~8절
# TermDocumentMatrix : [Term, Doc]
myCorpusPrepro_term
# (2) Corpus -> 평서문 변환 : matrix -> data.frame 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
str(myTerm_df)
myTerm_df[c(1:10), 1]
## 7. 단어 빈도수 구하기
# (1) 단어 빈도수 내림차순 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
w_name <- names(wordResult)
w_name[1:10] # 단어이름
wordResult[1:10] # 출현빈도
# (2) 불용어 제거 : 의미없는 단어 제거
# [단계1] 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, tolower) # 소문자 변경
myCorpusPrepro <-tm_map(myCorpusPrepro, removeWords, c(stopwords('english'),'사용','하기')) # 불용어제거
# [단계2] 단어문서 행렬 생성
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(4,16))) # 2절~8절
# [단계3] 말뭉치 -> 평서문
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
# [단계4] 내림정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
## 8. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal, family="malgun")
